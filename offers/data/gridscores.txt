optimizer rmsprop batch size 16 epoch 800 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 16 epoch 800 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 16 epoch 800 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 16 epoch 800 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 16 epoch 800 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer rmsprop batch size 16 epoch 800 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 16 epoch 800 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.03703703731298447 accuracy 0.03703703731298447
optimizer rmsprop batch size 16 epoch 800 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 16 epoch 800 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.03703703731298447 accuracy 0.03703703731298447
optimizer rmsprop batch size 16 epoch 1000 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 16 epoch 1000 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 16 epoch 1000 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 16 epoch 1000 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 16 epoch 1000 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 16 epoch 1000 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.03703703731298447 accuracy 0.03703703731298447
optimizer rmsprop batch size 16 epoch 1000 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.03703703731298447 accuracy 0.03703703731298447
optimizer rmsprop batch size 16 epoch 1000 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 16 epoch 1000 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.03703703731298447 accuracy 0.03703703731298447
optimizer rmsprop batch size 16 epoch 1200 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 16 epoch 1200 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 16 epoch 1200 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer rmsprop batch size 16 epoch 1200 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 16 epoch 1200 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.03703703731298447 accuracy 0.03703703731298447
optimizer rmsprop batch size 16 epoch 1200 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.03703703731298447 accuracy 0.03703703731298447
optimizer rmsprop batch size 16 epoch 1200 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.03703703731298447 accuracy 0.03703703731298447
optimizer rmsprop batch size 16 epoch 1200 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.03703703731298447 accuracy 0.03703703731298447
optimizer rmsprop batch size 16 epoch 1200 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.03703703731298447 accuracy 0.03703703731298447
optimizer rmsprop batch size 32 epoch 800 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 32 epoch 800 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 32 epoch 800 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 32 epoch 800 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 32 epoch 800 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 32 epoch 800 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 32 epoch 800 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 32 epoch 800 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 32 epoch 800 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 32 epoch 1000 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 32 epoch 1000 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 32 epoch 1000 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 32 epoch 1000 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 32 epoch 1000 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 32 epoch 1000 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 32 epoch 1000 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 32 epoch 1000 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 32 epoch 1000 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 32 epoch 1200 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 32 epoch 1200 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 32 epoch 1200 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 32 epoch 1200 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 32 epoch 1200 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer rmsprop batch size 32 epoch 1200 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer rmsprop batch size 32 epoch 1200 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 32 epoch 1200 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 32 epoch 1200 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 64 epoch 800 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 64 epoch 800 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 64 epoch 800 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 64 epoch 800 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 64 epoch 800 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer rmsprop batch size 64 epoch 800 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 64 epoch 800 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 64 epoch 800 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 64 epoch 800 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 64 epoch 1000 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 64 epoch 1000 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer rmsprop batch size 64 epoch 1000 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 64 epoch 1000 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer rmsprop batch size 64 epoch 1000 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 64 epoch 1000 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 64 epoch 1000 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer rmsprop batch size 64 epoch 1000 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 64 epoch 1000 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 64 epoch 1200 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 64 epoch 1200 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer rmsprop batch size 64 epoch 1200 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer rmsprop batch size 64 epoch 1200 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 64 epoch 1200 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 64 epoch 1200 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 64 epoch 1200 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer rmsprop batch size 64 epoch 1200 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer rmsprop batch size 64 epoch 1200 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer sgd batch size 16 epoch 800 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.6481481194496155 accuracy 0.6481481194496155
optimizer sgd batch size 16 epoch 800 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.5925925970077515 accuracy 0.5925925970077515
optimizer sgd batch size 16 epoch 800 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.46296295523643494 accuracy 0.46296295523643494
optimizer sgd batch size 16 epoch 800 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.6481481194496155 accuracy 0.6481481194496155
optimizer sgd batch size 16 epoch 800 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.6296296119689941 accuracy 0.6296296119689941
optimizer sgd batch size 16 epoch 800 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.6296296119689941 accuracy 0.6296296119689941
optimizer sgd batch size 16 epoch 800 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer sgd batch size 16 epoch 800 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer sgd batch size 16 epoch 800 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.6481481194496155 accuracy 0.6481481194496155
optimizer sgd batch size 16 epoch 1000 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer sgd batch size 16 epoch 1000 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.6296296119689941 accuracy 0.6296296119689941
optimizer sgd batch size 16 epoch 1000 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.5555555820465088 accuracy 0.5555555820465088
optimizer sgd batch size 16 epoch 1000 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer sgd batch size 16 epoch 1000 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer sgd batch size 16 epoch 1000 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer sgd batch size 16 epoch 1000 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer sgd batch size 16 epoch 1000 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer sgd batch size 16 epoch 1000 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer sgd batch size 16 epoch 1200 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer sgd batch size 16 epoch 1200 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer sgd batch size 16 epoch 1200 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.6296296119689941 accuracy 0.6296296119689941
optimizer sgd batch size 16 epoch 1200 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer sgd batch size 16 epoch 1200 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer sgd batch size 16 epoch 1200 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer sgd batch size 16 epoch 1200 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer sgd batch size 16 epoch 1200 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer sgd batch size 16 epoch 1200 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer sgd batch size 32 epoch 800 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.12962962687015533 accuracy 0.12962962687015533
optimizer sgd batch size 32 epoch 800 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.14814814925193787 accuracy 0.14814814925193787
optimizer sgd batch size 32 epoch 800 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.18518517911434174 accuracy 0.18518517911434174
optimizer sgd batch size 32 epoch 800 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.2222222238779068 accuracy 0.2222222238779068
optimizer sgd batch size 32 epoch 800 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.14814814925193787 accuracy 0.14814814925193787
optimizer sgd batch size 32 epoch 800 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.12962962687015533 accuracy 0.12962962687015533
optimizer sgd batch size 32 epoch 800 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.29629629850387573 accuracy 0.29629629850387573
optimizer sgd batch size 32 epoch 800 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.1666666716337204 accuracy 0.1666666716337204
optimizer sgd batch size 32 epoch 800 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.09259258955717087 accuracy 0.09259258955717087
optimizer sgd batch size 32 epoch 1000 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.24074074625968933 accuracy 0.24074074625968933
optimizer sgd batch size 32 epoch 1000 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.09259258955717087 accuracy 0.09259258955717087
optimizer sgd batch size 32 epoch 1000 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.1111111119389534 accuracy 0.1111111119389534
optimizer sgd batch size 32 epoch 1000 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.5370370149612427 accuracy 0.5370370149612427
optimizer sgd batch size 32 epoch 1000 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.14814814925193787 accuracy 0.14814814925193787
optimizer sgd batch size 32 epoch 1000 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.20370370149612427 accuracy 0.20370370149612427
optimizer sgd batch size 32 epoch 1000 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.6296296119689941 accuracy 0.6296296119689941
optimizer sgd batch size 32 epoch 1000 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.5555555820465088 accuracy 0.5555555820465088
optimizer sgd batch size 32 epoch 1000 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.48148149251937866 accuracy 0.48148149251937866
optimizer sgd batch size 32 epoch 1200 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.5370370149612427 accuracy 0.5370370149612427
optimizer sgd batch size 32 epoch 1200 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.42592594027519226 accuracy 0.42592594027519226
optimizer sgd batch size 32 epoch 1200 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.14814814925193787 accuracy 0.14814814925193787
optimizer sgd batch size 32 epoch 1200 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.5925925970077515 accuracy 0.5925925970077515
optimizer sgd batch size 32 epoch 1200 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.5740740895271301 accuracy 0.5740740895271301
optimizer sgd batch size 32 epoch 1200 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.48148149251937866 accuracy 0.48148149251937866
optimizer sgd batch size 32 epoch 1200 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.6296296119689941 accuracy 0.6296296119689941
optimizer sgd batch size 32 epoch 1200 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.6111111044883728 accuracy 0.6111111044883728
optimizer sgd batch size 32 epoch 1200 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.5925925970077515 accuracy 0.5925925970077515
optimizer sgd batch size 64 epoch 800 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.18518517911434174 accuracy 0.18518517911434174
optimizer sgd batch size 64 epoch 800 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.2222222238779068 accuracy 0.2222222238779068
optimizer sgd batch size 64 epoch 800 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.12962962687015533 accuracy 0.12962962687015533
optimizer sgd batch size 64 epoch 800 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.2222222238779068 accuracy 0.2222222238779068
optimizer sgd batch size 64 epoch 800 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.29629629850387573 accuracy 0.29629629850387573
optimizer sgd batch size 64 epoch 800 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.1666666716337204 accuracy 0.1666666716337204
optimizer sgd batch size 64 epoch 800 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.1666666716337204 accuracy 0.1666666716337204
optimizer sgd batch size 64 epoch 800 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.18518517911434174 accuracy 0.18518517911434174
optimizer sgd batch size 64 epoch 800 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.20370370149612427 accuracy 0.20370370149612427
optimizer sgd batch size 64 epoch 1000 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.1666666716337204 accuracy 0.1666666716337204
optimizer sgd batch size 64 epoch 1000 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.14814814925193787 accuracy 0.14814814925193787
optimizer sgd batch size 64 epoch 1000 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.2222222238779068 accuracy 0.2222222238779068
optimizer sgd batch size 64 epoch 1000 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.20370370149612427 accuracy 0.20370370149612427
optimizer sgd batch size 64 epoch 1000 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.14814814925193787 accuracy 0.14814814925193787
optimizer sgd batch size 64 epoch 1000 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.12962962687015533 accuracy 0.12962962687015533
optimizer sgd batch size 64 epoch 1000 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.1666666716337204 accuracy 0.1666666716337204
optimizer sgd batch size 64 epoch 1000 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.20370370149612427 accuracy 0.20370370149612427
optimizer sgd batch size 64 epoch 1000 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.20370370149612427 accuracy 0.20370370149612427
optimizer sgd batch size 64 epoch 1200 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.07407407462596893 accuracy 0.07407407462596893
optimizer sgd batch size 64 epoch 1200 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.0555555559694767 accuracy 0.0555555559694767
optimizer sgd batch size 64 epoch 1200 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.0555555559694767 accuracy 0.0555555559694767
optimizer sgd batch size 64 epoch 1200 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.2222222238779068 accuracy 0.2222222238779068
optimizer sgd batch size 64 epoch 1200 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.1111111119389534 accuracy 0.1111111119389534
optimizer sgd batch size 64 epoch 1200 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.1666666716337204 accuracy 0.1666666716337204
optimizer sgd batch size 64 epoch 1200 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.1666666716337204 accuracy 0.1666666716337204
optimizer sgd batch size 64 epoch 1200 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.12962962687015533 accuracy 0.12962962687015533
optimizer sgd batch size 64 epoch 1200 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.1111111119389534 accuracy 0.1111111119389534
optimizer adam batch size 16 epoch 800 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 16 epoch 800 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer adam batch size 16 epoch 800 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer adam batch size 16 epoch 800 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 16 epoch 800 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 16 epoch 800 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer adam batch size 16 epoch 800 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 16 epoch 800 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer adam batch size 16 epoch 800 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 16 epoch 1000 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 16 epoch 1000 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 16 epoch 1000 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 16 epoch 1000 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 16 epoch 1000 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 16 epoch 1000 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 16 epoch 1000 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer adam batch size 16 epoch 1000 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 16 epoch 1000 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 16 epoch 1200 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 16 epoch 1200 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 16 epoch 1200 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer adam batch size 16 epoch 1200 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 16 epoch 1200 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 16 epoch 1200 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 16 epoch 1200 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 16 epoch 1200 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 16 epoch 1200 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 32 epoch 800 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 800 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 32 epoch 800 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.8148148059844971 accuracy 0.8148148059844971
optimizer adam batch size 32 epoch 800 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 800 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 800 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 800 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 800 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 800 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 32 epoch 1000 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 1000 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 1000 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 32 epoch 1000 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer adam batch size 32 epoch 1000 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 32 epoch 1000 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer adam batch size 32 epoch 1000 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 1000 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 32 epoch 1000 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 1200 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 32 epoch 1200 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer adam batch size 32 epoch 1200 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer adam batch size 32 epoch 1200 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 32 epoch 1200 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 1200 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 1200 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 32 epoch 1200 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 32 epoch 1200 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 64 epoch 800 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 64 epoch 800 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer adam batch size 64 epoch 800 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 64 epoch 800 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 64 epoch 800 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 64 epoch 800 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7962962985038757 accuracy 0.7962962985038757
optimizer adam batch size 64 epoch 800 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 64 epoch 800 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer adam batch size 64 epoch 800 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 64 epoch 1000 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer adam batch size 64 epoch 1000 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 64 epoch 1000 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 64 epoch 1000 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 64 epoch 1000 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 64 epoch 1000 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 64 epoch 1000 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 64 epoch 1000 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 64 epoch 1000 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 64 epoch 1200 layer [128, 64] dropout 0.1 activation sigmoid val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer adam batch size 64 epoch 1200 layer [128, 64] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 64 epoch 1200 layer [128, 64] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 64 epoch 1200 layer [256, 128] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 64 epoch 1200 layer [256, 128] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 64 epoch 1200 layer [256, 128] dropout 0.5 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 64 epoch 1200 layer [512, 256] dropout 0.1 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
optimizer adam batch size 64 epoch 1200 layer [512, 256] dropout 0.3 activation sigmoid val accuracy 0.7777777910232544 accuracy 0.7777777910232544
optimizer adam batch size 64 epoch 1200 layer [512, 256] dropout 0.5 activation sigmoid val accuracy 0.7592592835426331 accuracy 0.7592592835426331
