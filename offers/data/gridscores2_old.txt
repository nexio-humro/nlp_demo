optimizer sgd batch size 32 epoch 800 layer [128, 64] dropout 0.1 activation softmax val accuracy 0.6111111044883728 accuracy 0.6111111044883728
optimizer sgd batch size 32 epoch 800 layer [128, 64] dropout 0.3 activation softmax val accuracy 0.5740740895271301 accuracy 0.5740740895271301
optimizer sgd batch size 32 epoch 800 layer [128, 64] dropout 0.5 activation softmax val accuracy 0.46296295523643494 accuracy 0.46296295523643494
optimizer sgd batch size 32 epoch 800 layer [256, 128] dropout 0.1 activation softmax val accuracy 0.6481481194496155 accuracy 0.6481481194496155
optimizer sgd batch size 32 epoch 800 layer [256, 128] dropout 0.3 activation softmax val accuracy 0.5740740895271301 accuracy 0.5740740895271301
optimizer sgd batch size 32 epoch 800 layer [256, 128] dropout 0.5 activation softmax val accuracy 0.5925925970077515 accuracy 0.5925925970077515
optimizer sgd batch size 32 epoch 1000 layer [128, 64] dropout 0.1 activation softmax val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer sgd batch size 32 epoch 1000 layer [128, 64] dropout 0.3 activation softmax val accuracy 0.5740740895271301 accuracy 0.5740740895271301
optimizer sgd batch size 32 epoch 1000 layer [128, 64] dropout 0.5 activation softmax val accuracy 0.6296296119689941 accuracy 0.6296296119689941
optimizer sgd batch size 32 epoch 1000 layer [256, 128] dropout 0.1 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer sgd batch size 32 epoch 1000 layer [256, 128] dropout 0.3 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer sgd batch size 32 epoch 1000 layer [256, 128] dropout 0.5 activation softmax val accuracy 0.6111111044883728 accuracy 0.6111111044883728
optimizer sgd batch size 32 epoch 1200 layer [128, 64] dropout 0.1 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer sgd batch size 32 epoch 1200 layer [128, 64] dropout 0.3 activation softmax val accuracy 0.6296296119689941 accuracy 0.6296296119689941
optimizer sgd batch size 32 epoch 1200 layer [128, 64] dropout 0.5 activation softmax val accuracy 0.5740740895271301 accuracy 0.5740740895271301
optimizer sgd batch size 32 epoch 1200 layer [256, 128] dropout 0.1 activation softmax val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer sgd batch size 32 epoch 1200 layer [256, 128] dropout 0.3 activation softmax val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer sgd batch size 32 epoch 1200 layer [256, 128] dropout 0.5 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer sgd batch size 64 epoch 800 layer [128, 64] dropout 0.1 activation softmax val accuracy 0.2777777910232544 accuracy 0.2777777910232544
optimizer sgd batch size 64 epoch 800 layer [128, 64] dropout 0.3 activation softmax val accuracy 0.3333333432674408 accuracy 0.3333333432674408
optimizer sgd batch size 64 epoch 800 layer [128, 64] dropout 0.5 activation softmax val accuracy 0.25925925374031067 accuracy 0.25925925374031067
optimizer sgd batch size 64 epoch 800 layer [256, 128] dropout 0.1 activation softmax val accuracy 0.40740740299224854 accuracy 0.40740740299224854
optimizer sgd batch size 64 epoch 800 layer [256, 128] dropout 0.3 activation softmax val accuracy 0.37037035822868347 accuracy 0.37037035822868347
optimizer sgd batch size 64 epoch 800 layer [256, 128] dropout 0.5 activation softmax val accuracy 0.35185185074806213 accuracy 0.35185185074806213
optimizer sgd batch size 64 epoch 1000 layer [128, 64] dropout 0.1 activation softmax val accuracy 0.3333333432674408 accuracy 0.3333333432674408
optimizer sgd batch size 64 epoch 1000 layer [128, 64] dropout 0.3 activation softmax val accuracy 0.31481480598449707 accuracy 0.31481480598449707
optimizer sgd batch size 64 epoch 1000 layer [128, 64] dropout 0.5 activation softmax val accuracy 0.2777777910232544 accuracy 0.2777777910232544
optimizer sgd batch size 64 epoch 1000 layer [256, 128] dropout 0.1 activation softmax val accuracy 0.48148149251937866 accuracy 0.48148149251937866
optimizer sgd batch size 64 epoch 1000 layer [256, 128] dropout 0.3 activation softmax val accuracy 0.4444444477558136 accuracy 0.4444444477558136
optimizer sgd batch size 64 epoch 1000 layer [256, 128] dropout 0.5 activation softmax val accuracy 0.3888888955116272 accuracy 0.3888888955116272
optimizer sgd batch size 64 epoch 1200 layer [128, 64] dropout 0.1 activation softmax val accuracy 0.48148149251937866 accuracy 0.48148149251937866
optimizer sgd batch size 64 epoch 1200 layer [128, 64] dropout 0.3 activation softmax val accuracy 0.3888888955116272 accuracy 0.3888888955116272
optimizer sgd batch size 64 epoch 1200 layer [128, 64] dropout 0.5 activation softmax val accuracy 0.37037035822868347 accuracy 0.37037035822868347
optimizer sgd batch size 64 epoch 1200 layer [256, 128] dropout 0.1 activation softmax val accuracy 0.5925925970077515 accuracy 0.5925925970077515
optimizer sgd batch size 64 epoch 1200 layer [256, 128] dropout 0.3 activation softmax val accuracy 0.5370370149612427 accuracy 0.5370370149612427
optimizer sgd batch size 64 epoch 1200 layer [256, 128] dropout 0.5 activation softmax val accuracy 0.5 accuracy 0.5
optimizer rmsprop batch size 32 epoch 800 layer [128, 64] dropout 0.1 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer rmsprop batch size 32 epoch 800 layer [128, 64] dropout 0.3 activation softmax val accuracy 0.6481481194496155 accuracy 0.6481481194496155
optimizer rmsprop batch size 32 epoch 800 layer [128, 64] dropout 0.5 activation softmax val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer rmsprop batch size 32 epoch 800 layer [256, 128] dropout 0.1 activation softmax val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer rmsprop batch size 32 epoch 800 layer [256, 128] dropout 0.3 activation softmax val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer rmsprop batch size 32 epoch 800 layer [256, 128] dropout 0.5 activation softmax val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer rmsprop batch size 32 epoch 1000 layer [128, 64] dropout 0.1 activation softmax val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer rmsprop batch size 32 epoch 1000 layer [128, 64] dropout 0.3 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer rmsprop batch size 32 epoch 1000 layer [128, 64] dropout 0.5 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer rmsprop batch size 32 epoch 1000 layer [256, 128] dropout 0.1 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer rmsprop batch size 32 epoch 1000 layer [256, 128] dropout 0.3 activation softmax val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer rmsprop batch size 32 epoch 1000 layer [256, 128] dropout 0.5 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer rmsprop batch size 32 epoch 1200 layer [128, 64] dropout 0.1 activation softmax val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer rmsprop batch size 32 epoch 1200 layer [128, 64] dropout 0.3 activation softmax val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer rmsprop batch size 32 epoch 1200 layer [128, 64] dropout 0.5 activation softmax val accuracy 0.7222222089767456 accuracy 0.7222222089767456
optimizer rmsprop batch size 32 epoch 1200 layer [256, 128] dropout 0.1 activation softmax val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer rmsprop batch size 32 epoch 1200 layer [256, 128] dropout 0.3 activation softmax val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer rmsprop batch size 32 epoch 1200 layer [256, 128] dropout 0.5 activation softmax val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer rmsprop batch size 64 epoch 800 layer [128, 64] dropout 0.1 activation softmax val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer rmsprop batch size 64 epoch 800 layer [128, 64] dropout 0.3 activation softmax val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer rmsprop batch size 64 epoch 800 layer [128, 64] dropout 0.5 activation softmax val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer rmsprop batch size 64 epoch 800 layer [256, 128] dropout 0.1 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer rmsprop batch size 64 epoch 800 layer [256, 128] dropout 0.3 activation softmax val accuracy 0.7407407164573669 accuracy 0.7407407164573669
optimizer rmsprop batch size 64 epoch 800 layer [256, 128] dropout 0.5 activation softmax val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer rmsprop batch size 64 epoch 1000 layer [128, 64] dropout 0.1 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer rmsprop batch size 64 epoch 1000 layer [128, 64] dropout 0.3 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
optimizer rmsprop batch size 64 epoch 1000 layer [128, 64] dropout 0.5 activation softmax val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer rmsprop batch size 64 epoch 1000 layer [256, 128] dropout 0.1 activation softmax val accuracy 0.7037037014961243 accuracy 0.7037037014961243
optimizer rmsprop batch size 64 epoch 1000 layer [256, 128] dropout 0.3 activation softmax val accuracy 0.6666666865348816 accuracy 0.6666666865348816
optimizer rmsprop batch size 64 epoch 1000 layer [256, 128] dropout 0.5 activation softmax val accuracy 0.6851851940155029 accuracy 0.6851851940155029
